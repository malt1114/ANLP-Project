{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ea4a4c3-44d9-415d-855d-afa58addf0be",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a00071e-4324-4de2-947f-d0385b367325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ruben\\\\OneDrive\\\\Dokumenter\\\\GitHub\\\\ANLP-Project'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.chdir('..')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6000c37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ruben\\\\OneDrive\\\\Dokumenter\\\\GitHub\\\\ANLP-Project'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddda24b-27a5-40cc-93d9-6a0fbf3c2de1",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6abbde7-dc63-471e-8d5f-5a31201c1963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ruben\\AppData\\Local\\Temp\\ipykernel_38216\\2098325508.py:12: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange, tqdm\n"
     ]
    }
   ],
   "source": [
    "from scripts.model import CharBiLSTM, prepare_data\n",
    "from scripts.data import create_data_loader, load_data\n",
    "from scripts.preprocessing import get_typoglycemia_modified_data, sentence_tokennizer, tokenize_dataframe, get_max_length\n",
    "from scripts.baseline import get_base_line_score\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import editdistance\n",
    "from tqdm.autonotebook import trange, tqdm\n",
    "import random \n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1176928e-b58d-46cb-a601-262e9fecfd37",
   "metadata": {},
   "source": [
    "# Hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd431642-6d32-4817-8b8c-f89994953e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from model_train_easy import device, complexity_level, max_length, batch_size, input_size, hidden_size, output_size, num_layers, complexity_level, model, loss_function, optimizer, epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ac98230-142d-40ba-9c8c-1fc24e06329e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0), 'Easy')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device, complexity_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a58c359c-3e09-4364-9a7d-a6ac8de05764",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, validation_loader, test_loader = prepare_data(complexity_level = complexity_level,\n",
    "                                                            max_length = max_length,\n",
    "                                                            batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df6bdb0d-0e96-4f74-b6b2-572c8b485716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 150]) torch.Size([512, 150])\n",
      "(array([-1,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26], dtype=int64), array([42356,  2616,   482,   898,  1031,  3466,   640,   484,  1478,\n",
      "        2189,    89,   215,  1187,   840,  2189,  2079,   589,    20,\n",
      "        1824,  1943,  2417,   739,   237,   518,    59,   466,  5749],\n",
      "      dtype=int64))\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "for sample in train_loader:\n",
    "    X, y = sample\n",
    "    print(X.shape, y.shape)\n",
    "    print(np.unique(y.cpu(), return_counts=True))\n",
    "    print(X.device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eeb2c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ruben\\\\OneDrive\\\\Dokumenter\\\\GitHub\\\\ANLP-Project'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8479efdc-13ab-473f-8de5-76b7a0f3ff9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharBiLSTM(\n",
       "  (lstm): LSTM(1, 512, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=1024, out_features=28, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if complexity_level.lower() == 'easy':\n",
    "    PATH = f'models/{complexity_level.lower()}/model_462.pt'\n",
    "else:\n",
    "    PATH = f'models/{complexity_level.lower()}/model_270.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(PATH, weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adb9886f-f783-4894-9471-a33d36e5120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_char(index, target = True):\n",
    "    if target == False:\n",
    "        index = round(index*100, 0)\n",
    "    index = int(index)\n",
    "    if 1 <= index <= 25:\n",
    "        return chr(index + ord('a') - 1), index\n",
    "    elif index == 26:\n",
    "        return ' ', index\n",
    "    else:\n",
    "        return \"_\", index  # for all other values, return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31edfb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import editdistance\n",
    "\n",
    "def get_metrics(loader, model, loader_str, output_file=\"output.csv\", sample_size=10):\n",
    "    preds = []\n",
    "    labels = []\n",
    "    inputs = []  # To store input sequences\n",
    "    \n",
    "    model.eval()\n",
    "    loss = 0.0\n",
    "    csv_data = [[\"Input Word\", \"Label_Input_Editdistance\", \"Label Word\", \"Pred_label_Editdistance\", \"Predicted Word\"]]  # CSV header\n",
    "    sentence_label = []\n",
    "    sentence_pred = []\n",
    "    sentence_input = []  # To store input words\n",
    "    all_edit_distances = []\n",
    "    print_count = 0\n",
    "    all_typo_edit_distances = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, position=3, leave=False):\n",
    "            typo_batch, sentence_batch = batch  # typo_batch is the input, sentence_batch is the target sequence\n",
    "            typo_batch = typo_batch.to(device)\n",
    "            sentence_batch =sentence_batch.to(device)\n",
    "            \n",
    "            sentence_batch = sentence_batch.reshape(-1, max_length, 1)\n",
    "            typo_batch = typo_batch.reshape(-1, max_length, 1)\n",
    "            \n",
    "            # print(sentence_batch[0])\n",
    "            # for index in range(len(sentence_batch)):\n",
    "            #     sentence = sentence_batch[index].cpu().numpy().reshape(-1)\n",
    "            #     typo = typo_batch[index].cpu().numpy().reshape(-1)\n",
    "\n",
    "            #     for i, value in enumerate(sentence):\n",
    "            #         if value == -1:\n",
    "            #             sentence[i] = 26\n",
    "            #             break\n",
    "\n",
    "            #     for i, value in enumerate(typo):\n",
    "            #         if value == -1:\n",
    "            #             typo[i] = 26\n",
    "            #             break\n",
    "                \n",
    "            #     sentence_batch[index] = torch.tensor(sentence).reshape(max_length, 1).to(device)\n",
    "            #     typo_batch[index] = torch.tensor(typo).reshape(max_length, 1).to(device)\n",
    "            # print(sentence_batch[0])\n",
    "                \n",
    "            # print(typo_batch.shape)\n",
    "            y = model.forward(typo_batch, train=False)\n",
    "\n",
    "            \n",
    "            sentence_batch_loss = sentence_batch.reshape(-1)\n",
    "            # Calculate and accumulate loss\n",
    "            loss_batch = loss_function(y, sentence_batch_loss)\n",
    "            loss += loss_batch.item()\n",
    "            \n",
    "            \n",
    "\n",
    "            word_label = []\n",
    "            word_pred = []\n",
    "            word_input = []\n",
    "            \n",
    "            y = y.reshape(-1, max_length, output_size)\n",
    "            ### Make it work for the nabsew dimensionality\n",
    "            for sentence_index in range(len(y)):\n",
    "                sentence = y[sentence_index]\n",
    "                sentence_labels = sentence_batch[sentence_index]\n",
    "                sentence_inputs = typo_batch[sentence_index]\n",
    "                \n",
    "                sentence_edit_distances = []\n",
    "                typo_sentence_edit_distances = []\n",
    "\n",
    "\n",
    "                # Get predictions, labels, and inputs in a flattened form\n",
    "                batch_preds = torch.argmax(sentence, dim=1).cpu().numpy().reshape(-1)\n",
    "                batch_labels = sentence_labels.cpu().numpy().reshape(-1)\n",
    "                batch_inputs = sentence_inputs.cpu().numpy().reshape(-1)\n",
    "                \n",
    "                # Append these results for later accuracy and F1 calculations\n",
    "                preds.extend(batch_preds)\n",
    "                labels.extend(batch_labels)\n",
    "                inputs.extend(batch_inputs)\n",
    "                # print(\"label_char\", \"pred_char\", \"input_char\")\n",
    "                \n",
    "                # Process each element in the batch\n",
    "                for label_idx in range(len(batch_preds)):\n",
    "                    label_char, label_index_ = index_to_char(batch_labels[label_idx])\n",
    "                    pred_char, pred_index_ = index_to_char(batch_preds[label_idx])\n",
    "                    input_char, input_index_ = index_to_char(batch_inputs[label_idx],target = False)\n",
    "\n",
    "                    # print(f\"label_index_: {label_index_}, pred_index_: {pred_index_}, input_index_: {input_index_}\")\n",
    "\n",
    "                    # Ignore padding characters\n",
    "                    if label_char != \"_\":\n",
    "                        word_label.append(label_char)\n",
    "                    if pred_char != \"_\":\n",
    "                        word_pred.append(pred_char)\n",
    "                    if input_char != \"_\":\n",
    "                        word_input.append(input_char)\n",
    "                    \n",
    "                    # If end of a word (detected by space), append word to csv_data\n",
    "                    if label_char == \" \":\n",
    "                        # Join and append the completed word to csv data\n",
    "                        distance = editdistance.eval(\"\".join(word_pred), \"\".join(word_label)) / len(\"\".join(word_label))\n",
    "                        sentence_edit_distances.append(distance)\n",
    "\n",
    "                        distance_typo_label = editdistance.eval(\"\".join(word_input), \"\".join(word_label)) / len(\"\".join(word_label))\n",
    "                        typo_sentence_edit_distances.append(distance_typo_label)\n",
    "\n",
    "\n",
    "                        if print_count == 0:\n",
    "                            print(f\"Word input: '{word_input}' Word pred: {word_pred} Editdistance: {distance_typo_label}\")\n",
    "                            print(f\"Word label: {''.join(word_label)} Word pred: {''.join(word_pred)} Editdistance: {distance}\\n\")\n",
    "\n",
    "                        csv_data.append([\"\".join(word_input), distance_typo_label, \"\".join(word_label), distance, \"\".join(word_pred)])\n",
    "                        # Clear the lists for the next word\n",
    "                        word_label = []\n",
    "                        word_pred = []\n",
    "                        word_input = []\n",
    "                \n",
    "                try:\n",
    "                    distance = editdistance.eval(\"\".join(word_pred), \"\".join(word_label)) / len(\"\".join(word_label))\n",
    "                    sentence_edit_distances.append(distance)\n",
    "\n",
    "                    distance_typo_label = editdistance.eval(\"\".join(word_input), \"\".join(word_label)) / len(\"\".join(word_label))\n",
    "                    typo_sentence_edit_distances.append(distance_typo_label)\n",
    "                    csv_data.append([\"\".join(word_input), distance_typo_label, \"\".join(word_label), distance, \"\".join(word_pred)])\n",
    "                    \n",
    "                except:\n",
    "                    pass\n",
    "                    # print(f\"word_input '{word_input}'\", \"word_pred\", word_pred)\n",
    "\n",
    "                all_edit_distances.extend(sentence_edit_distances)\n",
    "                all_typo_edit_distances.extend(typo_sentence_edit_distances)\n",
    "\n",
    "                if print_count == 0:\n",
    "                    print(f\"Word input: '{word_input}' Word pred: {word_pred} Editdistance: {distance_typo_label}\")\n",
    "                    print(f\"Word label: {''.join(word_label)} Word pred: {''.join(word_pred)} Editdistance: {distance}\\n\")\n",
    "                    # print(f\"Word label: {\"\".join(word_label)} Word pred: {\"\".join(word_pred)} Editdistance: {distance}\\n\")                   \n",
    "                    print(f\"All sentence edit distances\", sentence_edit_distances)\n",
    "                    print(f\"Average Normalized Edit Distance Pred-Label: {sum(sentence_edit_distances) / len(sentence_edit_distances):.4f}\")\n",
    "                    print(f\"Average Normalized Edit Distance Input-Label: {sum(typo_sentence_edit_distances) / len(typo_sentence_edit_distances):.4f}\")\n",
    "                word_label = []\n",
    "                word_pred = []\n",
    "                word_input = []\n",
    "                print_count += 1\n",
    "\n",
    "    # Write data to CSV file\n",
    "    with open(output_file, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(csv_data)\n",
    "    \n",
    "    # Display a subset of rows in the old format for console output\n",
    "    print(f\"\\nDisplaying the first {sample_size} rows from the output file:\\n\")\n",
    "    print(\"Input Word      | Label Word      | Predicted Word\")\n",
    "    print(\"-\" * 50)\n",
    "    for row in csv_data[1:sample_size + 1]:  # Skip header row, take only `sample_size` rows\n",
    "        print(f\"{row[0]:<15} | {row[2]:<15} | {row[4]}\")\n",
    "    \n",
    "    # Compute and display metrics\n",
    "    loss_avg = loss / len(loader)\n",
    "    print(f\"\\n{loader_str} Loss: {loss_avg:.4f}\")\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "\n",
    "\n",
    "    # # ADD EDIT DISTANCE METRIC HERE\n",
    "    \n",
    "    \n",
    "    print(f\"\\n{loader_str} Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "    # Calculate average edit distance\n",
    "    avg_edit_distance = sum(all_edit_distances) / len(all_edit_distances)\n",
    "    avg_edit_distance_typo = sum(all_typo_edit_distances) / len(all_typo_edit_distances)\n",
    "    print(f\"\\n{loader_str} Average Normalized Edit Distance Pred-Label: {avg_edit_distance:.4f}\")\n",
    "    print(f\"\\n{loader_str} Average Normalized Edit Distance Typo-Label: {avg_edit_distance_typo:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14f76451-8a1f-4457-aba3-32bfccc076e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_print = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "835e22cb-97f5-4232-b739-32b39935cf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_metrics(train_loader, model, \"train\", \"train_predictions.csv\", n_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16f02c01-46d1-4249-ba0c-28021e49ecbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f8c853e215e4e93983ef9ca3aa31a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word input: '['i', 't', ' ']' Word pred: ['i', 't', ' '] Editdistance: 0.0\n",
      "Word label: it  Word pred: it  Editdistance: 0.0\n",
      "\n",
      "Word input: '['c', 'a', 'n', ' ']' Word pred: ['c', 'a', 'n', ' '] Editdistance: 0.0\n",
      "Word label: can  Word pred: can  Editdistance: 0.0\n",
      "\n",
      "Word input: '['b', 'e', ' ']' Word pred: ['b', 'e', ' '] Editdistance: 0.0\n",
      "Word label: be  Word pred: be  Editdistance: 0.0\n",
      "\n",
      "Word input: '['m', 'a', 'd', 'e', ' ']' Word pred: ['m', 'a', 'd', 'e', ' '] Editdistance: 0.0\n",
      "Word label: made  Word pred: made  Editdistance: 0.0\n",
      "\n",
      "Word input: '['b', 'y', ' ']' Word pred: ['b', 'y', ' '] Editdistance: 0.0\n",
      "Word label: by  Word pred: by  Editdistance: 0.0\n",
      "\n",
      "Word input: '['r', 'a', 'e', 't', 'n', 'i', 'c', 'g', ' ']' Word pred: ['r', 'e', 'a', 'c', 't', 'i', 'n', 'g', ' '] Editdistance: 0.4444444444444444\n",
      "Word label: reacting  Word pred: reacting  Editdistance: 0.0\n",
      "\n",
      "Word input: '['a', 'h', 'y', 'n', 'r', 'o', 'u', 'd', 's', ' ']' Word pred: ['a', 'y', ' ', 'o', 'h', 'y', 'n', 'u', 's', ' '] Editdistance: 0.3\n",
      "Word label: anhydrous  Word pred: ay ohynus  Editdistance: 0.6\n",
      "\n",
      "Word input: '['w', 'u', 'o', 'i', 'h', 't', 't', ' ']' Word pred: ['w', 'i', 't', 'h', 'o', 'u', 't', ' '] Editdistance: 0.625\n",
      "Word label: without  Word pred: without  Editdistance: 0.0\n",
      "\n",
      "Word input: '['a', 'n', 'y', ' ']' Word pred: ['a', 'n', 'y', ' '] Editdistance: 0.0\n",
      "Word label: any  Word pred: any  Editdistance: 0.0\n",
      "\n",
      "Word input: '['w', 'e', 't', 'a', 'r', ' ']' Word pred: ['w', 'a', 't', 'e', 'r', ' '] Editdistance: 0.3333333333333333\n",
      "Word label: water  Word pred: water  Editdistance: 0.0\n",
      "\n",
      "Word input: '['m', 'e', 'l', 'o', 'c', 'u', 'e', 'l', 's', ' ']' Word pred: ['m', 'e', 'l', 'e', 'l', 'l', 'l', 'e', 's', ' '] Editdistance: 0.4\n",
      "Word label: molecules  Word pred: melellles  Editdistance: 0.3\n",
      "\n",
      "Word input: '['a', 't', 'c', 'h', 't', 'e', 'a', 'd', ' ']' Word pred: ['a', 't', 't', 'a', 'c', 't', 'e', 'd', ' '] Editdistance: 0.4444444444444444\n",
      "Word label: attached  Word pred: attacted  Editdistance: 0.1111111111111111\n",
      "\n",
      "Word input: '['c', 'a', 'b', 'o', 'l', 't', ' ']' Word pred: ['c', 'o', 'l', 'a', 'l', 't', ' '] Editdistance: 0.2857142857142857\n",
      "Word label: cobalt  Word pred: colalt  Editdistance: 0.14285714285714285\n",
      "\n",
      "Word input: '['i', 'i', ' ']' Word pred: ['i', 'i', ' '] Editdistance: 0.0\n",
      "Word label: ii  Word pred: ii  Editdistance: 0.0\n",
      "\n",
      "Word input: '['o', 'x', 'i', 'd', 'e', ' ']' Word pred: ['o', 'x', 'i', 'd', 'e', ' '] Editdistance: 0.0\n",
      "Word label: oxide  Word pred: oxide  Editdistance: 0.0\n",
      "\n",
      "Word input: '['o', 'r', ' ']' Word pred: ['o', 'r', ' '] Editdistance: 0.0\n",
      "Word label: or  Word pred: or  Editdistance: 0.0\n",
      "\n",
      "Word input: '['c', 'b', 'a', 'l', 'o', 't', ' ']' Word pred: ['c', 'o', 'b', 'i', 'l', 't', ' '] Editdistance: 0.2857142857142857\n",
      "Word label: cobalt  Word pred: cobilt  Editdistance: 0.14285714285714285\n",
      "\n",
      "Word input: '['i', 'i', ' ']' Word pred: ['i', 'i', ' '] Editdistance: 0.0\n",
      "Word label: ii  Word pred: ii  Editdistance: 0.0\n",
      "\n",
      "Word input: '['c', 'h', 'l', 'r', 'd', 'o', 'i', 'e', ' ']' Word pred: ['c', 'h', 'l', 'o', 'l', 'i', 'l', 'e', ' '] Editdistance: 0.4444444444444444\n",
      "Word label: chloride  Word pred: chlolile  Editdistance: 0.2222222222222222\n",
      "\n",
      "Word input: '['w', 't', 'i', 'h', ' ']' Word pred: ['w', 'i', 't', 'h', ' '] Editdistance: 0.4\n",
      "Word label: with  Word pred: with  Editdistance: 0.0\n",
      "\n",
      "Word input: '['a', ' ']' Word pred: ['a', ' '] Editdistance: 0.0\n",
      "Word label: a  Word pred: a  Editdistance: 0.0\n",
      "\n",
      "Word input: '['s', 'a', 'r', 'e', 't', 'm', ' ']' Word pred: ['s', 't', 't', 'e', 'r', 'n', ' '] Editdistance: 0.2857142857142857\n",
      "Word label: stream  Word pred: sttern  Editdistance: 0.42857142857142855\n",
      "\n",
      "Word input: '['o', 'f', ' ']' Word pred: ['o', 'f', ' '] Editdistance: 0.0\n",
      "Word label: of  Word pred: of  Editdistance: 0.0\n",
      "\n",
      "Word input: '['h', 'g', 'r', 'e', 'o', 'd', 'y', 'n', ' ']' Word pred: ['h', 'y', 'd', 'g', 'y', 'r', 'y', 'n', ' '] Editdistance: 0.5555555555555556\n",
      "Word label: hydrogen  Word pred: hydgyryn  Editdistance: 0.4444444444444444\n",
      "\n",
      "Word input: '['f', 'o', 'i', 'u', 'r', 'l', 'd', 'e']' Word pred: ['g', 'o', 'r', 'u', 'i', 'i', 'u', 'd', 'a', 'a', 'a', 'a'] Editdistance: 0.5\n",
      "Word label: fluoride Word pred: goruiiudaaaa Editdistance: 1.25\n",
      "\n",
      "All sentence edit distances [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.3, 0.1111111111111111, 0.14285714285714285, 0.0, 0.0, 0.0, 0.14285714285714285, 0.0, 0.2222222222222222, 0.0, 0.0, 0.42857142857142855, 0.0, 0.4444444444444444, 1.25]\n",
      "Average Normalized Edit Distance Pred-Label: 0.1457\n",
      "Average Normalized Edit Distance Input-Label: 0.2122\n",
      "\n",
      "Displaying the first 20 rows from the output file:\n",
      "\n",
      "Input Word      | Label Word      | Predicted Word\n",
      "--------------------------------------------------\n",
      "it              | it              | it \n",
      "can             | can             | can \n",
      "be              | be              | be \n",
      "made            | made            | made \n",
      "by              | by              | by \n",
      "raetnicg        | reacting        | reacting \n",
      "ahynrouds       | anhydrous       | ay ohynus \n",
      "wuoihtt         | without         | without \n",
      "any             | any             | any \n",
      "wetar           | water           | water \n",
      "melocuels       | molecules       | melellles \n",
      "atchtead        | attached        | attacted \n",
      "cabolt          | cobalt          | colalt \n",
      "ii              | ii              | ii \n",
      "oxide           | oxide           | oxide \n",
      "or              | or              | or \n",
      "cbalot          | cobalt          | cobilt \n",
      "ii              | ii              | ii \n",
      "chlrdoie        | chloride        | chlolile \n",
      "wtih            | with            | with \n",
      "\n",
      "validation Loss: 2.5582\n",
      "\n",
      "validation Accuracy: 0.3929, F1 Score: 0.3661\n",
      "\n",
      "validation Average Normalized Edit Distance Pred-Label: 1.6643\n",
      "\n",
      "validation Average Normalized Edit Distance Typo-Label: 0.2018\n"
     ]
    }
   ],
   "source": [
    "get_metrics(validation_loader, model, \"validation\", \"validation_predictions.csv\", n_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac37a233-7f0f-4870-8ae7-fc163bef7c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_metrics(test_loader, model, \"test\", \"test_predictions.csv\", n_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099dcf39",
   "metadata": {},
   "source": [
    "# Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33cc9782-d57d-4cf2-9d25-d2c9cbbc07b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between Frequency and Avg_Edit_Distance: -0.0004949322775614075\n",
      "Correlation between Length and Avg_Edit_Distance: -0.08576716070146674\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Length</th>\n",
       "      <th>Avg_Edit_Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it</td>\n",
       "      <td>6243.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.571627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>can</td>\n",
       "      <td>655.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.082824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>be</td>\n",
       "      <td>1149.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.341456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>made</td>\n",
       "      <td>433.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.393072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>by</td>\n",
       "      <td>3630.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.404959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589002</th>\n",
       "      <td>rvceeor</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589004</th>\n",
       "      <td>civrex</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589010</th>\n",
       "      <td>uterus</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589012</th>\n",
       "      <td>pdurotres</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589015</th>\n",
       "      <td>vaiangl</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151464 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Word  Frequency  Length  Avg_Edit_Distance\n",
       "0              it     6243.0     2.0           0.571627\n",
       "1             can      655.0     3.0           0.082824\n",
       "2              be     1149.0     2.0           0.341456\n",
       "3            made      433.0     4.0           0.393072\n",
       "4              by     3630.0     2.0           0.404959\n",
       "...           ...        ...     ...                ...\n",
       "589002    rvceeor        1.0     7.0           9.714286\n",
       "589004     civrex        1.0     6.0           0.571429\n",
       "589010     uterus        1.0     6.0           0.285714\n",
       "589012  pdurotres        1.0     9.0           0.400000\n",
       "589015    vaiangl        1.0     7.0           0.000000\n",
       "\n",
       "[151464 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"validation_predictions.csv\")\n",
    "\"\"\"\n",
    "Input Word,Label_Input_Editdistance, Label Word,Pred_label_Editdistance, Predicted Word\n",
    "gunionsn ,0.4444444444444444,gunnison ,0.3333333333333333,ginnnoon \n",
    "\"\"\"\n",
    "# Calculate word frequency, length, and average edit distance\n",
    "df['Word'] = df['Input Word'].str.strip()\n",
    "df['Frequency'] = df.groupby('Word')['Word'].transform('count')\n",
    "df['Length'] = df['Word'].str.len()\n",
    "df['Avg_Edit_Distance'] = df.groupby('Word')['Pred_label_Editdistance'].transform('mean')\n",
    "\n",
    "# Drop duplicates to keep unique words\n",
    "df_unique = df[['Word', 'Frequency', 'Length', 'Avg_Edit_Distance']].drop_duplicates()\n",
    "\n",
    "# Calculate correlations\n",
    "freq_avg_edit_corr = df_unique['Frequency'].corr(df_unique['Avg_Edit_Distance'])\n",
    "length_avg_edit_corr = df_unique['Length'].corr(df_unique['Avg_Edit_Distance'])\n",
    "\n",
    "print(f\"Correlation between Frequency and Avg_Edit_Distance: {freq_avg_edit_corr}\")\n",
    "print(f\"Correlation between Length and Avg_Edit_Distance: {length_avg_edit_corr}\")\n",
    "\n",
    "df_unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe360b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
