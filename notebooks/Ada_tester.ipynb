{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "random.seed(42)\n",
    "import num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:49: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:51: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:55: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:49: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:51: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:55: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\adagi\\AppData\\Local\\Temp\\ipykernel_20276\\1657701731.py:49: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  x = re.sub('[^a-zA-Z\\s\\.]', '', x)\n",
      "C:\\Users\\adagi\\AppData\\Local\\Temp\\ipykernel_20276\\1657701731.py:51: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  x = re.sub('\\.{2,}', ' ', x)\n",
      "C:\\Users\\adagi\\AppData\\Local\\Temp\\ipykernel_20276\\1657701731.py:55: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  x = x = re.sub('\\s{2,}', ' ', x)\n"
     ]
    }
   ],
   "source": [
    "def shuffle_string(x: str) -> str:\n",
    "    s = x\n",
    "    start, end = s[0], s[-1]\n",
    "    s = list(s[1:-1])\n",
    "    random.shuffle(s)\n",
    "    s = ''.join(s)\n",
    "    s = start + s + end\n",
    "    return s\n",
    "\n",
    "\n",
    "def get_typoglycemia_modified_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # typo_easy = []\n",
    "    # typo_hard = []\n",
    "    typo = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row['split'].replace('.', '').split(' ')\n",
    "        # easy = row['Easy'].replace('.', '').split(' ')\n",
    "        # hard = row['Hard'].replace('.', '').split(' ')\n",
    "\n",
    "        # shuffle words\n",
    "        # easy = [shuffle_string(i) if len(i) > 3 else i for i in easy]\n",
    "        # hard = [shuffle_string(i) if len(i) > 3 else i for i in hard]\n",
    "        text = [shuffle_string(i) if len(i) > 3 else i for i in text]\n",
    "\n",
    "        # typo_easy.append(' '.join(easy))\n",
    "        # typo_hard.append(' '.join(hard))\n",
    "        typo.append(' '.join(text))\n",
    "\n",
    "    # df['Easy_Typo'] = typo_easy\n",
    "    # df['Hard_Typo'] = typo_hard\n",
    "    df['typoglycemia'] = typo\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def sentence_tokennizer(sentence: str) -> list:\n",
    "    # Remove all non-alphabet chars\n",
    "    regex = re.compile('[^a-zA-Z ]')\n",
    "    sentence = regex.sub('', sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.split(' ')\n",
    "    # Remove empty strings\n",
    "    sentence = [i for i in sentence if len(i) != 0]\n",
    "    return sentence\n",
    "\n",
    "def sentence_preproces(x:str) -> list:\n",
    "    #Remove all chars that is not a full stop, space or in the alphabet\n",
    "    x = re.sub('[^a-zA-Z\\s\\.]', '', x)\n",
    "    #Remove multiple dots\n",
    "    x = re.sub('\\.{2,}', ' ', x)\n",
    "    #Remove . in acronymns\n",
    "    x = re.sub(r'\\b([a-zA-Z]\\.){2,}[a-zA-Z]\\b', lambda y: y.group().replace('.', ''), x)\n",
    "    #Remove any lenght of spaces except 1\n",
    "    x = x = re.sub('\\s{2,}', ' ', x)\n",
    "    #Remove dits in relation to other signs \n",
    "#    x = x.re.sub('\\.-|-\\.|.\"|\".','',x) # not used right now\n",
    "    return x.strip()\n",
    "\n",
    "\n",
    "def char_to_index(char):\n",
    "    if 'a' <= char <= 'z':\n",
    "        return (ord(char) - ord('a') + 1)\n",
    "    if char == \" \":\n",
    "        # return ord(char)\n",
    "        return 26\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "def convert_sentence_to_char_sequence(sentences: pd.Series, max_length: int, target: bool) -> torch.Tensor:\n",
    "\n",
    "    sequences = np.zeros((len(sentences), max_length), dtype= np.float32) - 1\n",
    "    \n",
    "    #If target keep it as a categorical value (int)\n",
    "    if target:\n",
    "        sequences = np.zeros((len(sentences), max_length)) - 1\n",
    "\n",
    "    for sentence_idx, sentence in enumerate(sentences):\n",
    "        for char_idx, char in enumerate(sentence):\n",
    "            if char_idx < max_length:\n",
    "                sequences[sentence_idx, char_idx] = char_to_index(char.lower())\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    #If not target, make it a float\n",
    "    if target == False:\n",
    "        sequences = sequences/100\n",
    "\n",
    "    return torch.Tensor(sequences)\n",
    "\n",
    "def tokenize_dataframe(df: pd.DataFrame, complexity: str) -> pd.DataFrame:\n",
    "    df.loc[:, complexity] = df[complexity].apply(lambda x: ' '.join(sentence_tokennizer(x)))\n",
    "    df.loc[:, complexity + \"_Typo\"] = df[complexity + \"_Typo\"].apply(lambda x: ' '.join(sentence_tokennizer(x)))\n",
    "    return df\n",
    "\n",
    "def get_max_length(df: pd.DataFrame, complexity_level: str):\n",
    "    # Combine the relevant sentence columns\n",
    "    all_sentences = pd.concat([df[complexity_level], df[complexity_level + \"_Typo\"]])\n",
    "\n",
    "    lengths = all_sentences.str.len()\n",
    "\n",
    "    # Calculate statistics\n",
    "    max_length = lengths.max()\n",
    "    mean_length = lengths.mean()\n",
    "    std_length = lengths.std()\n",
    "    median_length = lengths.median()\n",
    "\n",
    "    # Calculate the five-number summary\n",
    "    min_length = lengths.min()\n",
    "    q1_length = lengths.quantile(0.25)  # First quartile\n",
    "    q3_length = lengths.quantile(0.75)  # Third quartile\n",
    "\n",
    "    # Print the five-number summary\n",
    "    print(\n",
    "        f\"Five-number summary: Min: {min_length}, Q1: {q1_length}, Median: {median_length}, Q3: {q3_length}, Max: {max_length}\")\n",
    "    print(f\"Mean: {mean_length}, Std Dev: {std_length}\")\n",
    "\n",
    "    # Plot the distribution of lengths\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(lengths, bins=30, kde=True, color='blue', stat='density', alpha=0.6)\n",
    "    plt.axvline(mean_length, color='red', linestyle='dashed', linewidth=1, label=f'Mean: {mean_length:.2f}')\n",
    "    plt.axvline(median_length, color='green', linestyle='dashed', linewidth=1, label=f'Median: {median_length:.2f}')\n",
    "    plt.axvline(q1_length, color='orange', linestyle='dashed', linewidth=1, label=f'Q1: {q1_length:.2f}')\n",
    "    plt.axvline(q3_length, color='purple', linestyle='dashed', linewidth=1, label=f'Q3: {q3_length:.2f}')\n",
    "\n",
    "    plt.title('Distribution of Sentence Lengths')\n",
    "    plt.xlabel('Length of Sentences')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return max_length\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     sentences = pd.Series([\"Hello world\", \"test sentence\"])\n",
    "#     # sentences = pd.Series([\"Hello world\"])\n",
    "#     tensor_output = convert_sentence_to_char_sequence(sentences, 30)\n",
    "#     print(tensor_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replicate rows\n",
    "def split_rows(row):\n",
    "    preprocessed = sentence_preproces(row['original'])\n",
    "    #split_rows = preprocessed.split(',').split('.')\n",
    "    split_rows = re.split(r'[.,]', preprocessed)\n",
    "    new_rows = pd.DataFrame({'original':row['original']*len(split_rows),'split': split_rows})\n",
    "    new_rows['split'].replace('', np.nan, inplace=True)\n",
    "    return new_rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numbers_to_words(sentence):\n",
    "    return re.sub(r'\\b\\d+\\b', lambda x: num2words(int(x.group())), sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.,  5., 12., 12., 15., 26., 23., 15., 18., 12.,  4.,  0., 26., 20.,\n",
       "         15.,  4.,  1., 25., 26.,  9., 19., 26.,  1., 26.,  7., 15., 15.,  4.,\n",
       "         26.,  4.,  1., 25., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1.],\n",
       "        [20.,  5., 19., 20., 26., 19.,  5., 14., 20.,  5., 14.,  3.,  5., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = pd.Series([\"Hello world. Today is a good day\", \"test sentence\"])\n",
    "# sentences = pd.Series([\"Hello world\"])\n",
    "tensor_output = convert_sentence_to_char_sequence(sentences, 50, True)\n",
    "tensor_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Today is a good day</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0\n",
       "0           Hello world\n",
       "1   Today is a good day"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_preproces(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/raw/sscorpus.gz\", sep=\"\\t\", names=[\"Hard\", \"Easy\", \"Similarity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_typoglycemia_data_file(similarity_threshold: float, file_path: str):\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\", names=[\"Hard\", \"Easy\", \"Similarity\"])\n",
    "    print(df.shape)\n",
    "    df = df[df[\"Similarity\"] <= similarity_threshold]\n",
    "    print(df.shape)\n",
    "\n",
    "    #shsf\n",
    "    \n",
    "    df_hard = pd.DataFrame(columns=['original'])#, 'preproces', 'split', 'typoglycemia'])\n",
    "    df_easy = pd.DataFrame(columns=['original'])#, 'preproces', 'split', 'typoglycemia'])\n",
    "    df_hard['original'] = df['Hard']\n",
    "    df_easy['original'] = df['Easy']\n",
    "\n",
    "    #Split sentence at full stop and clean the sentences\n",
    "    df_hard = pd.concat([split_rows(row) for _, row in df_hard.iterrows()], ignore_index=True)\n",
    "    df_hard = df_hard[df_hard['split'].notna()]  \n",
    "\n",
    "\n",
    "    df_hard['typoglycemia'] = df_hard['split'].apply(convert_numbers_to_words)\n",
    "    df_hard = get_typoglycemia_modified_data(df_hard)\n",
    "    df_hard.reset_index(inplace=True, drop=True)\n",
    "    df_hard.to_csv(\"../data/processed/sscorpus_hard2.csv\", index=False)\n",
    "\n",
    "    df_easy = pd.concat([split_rows(row) for _, row in df_easy.iterrows()], ignore_index=True)\n",
    "    df_easy = df_easy[df_easy['split'].notna()]\n",
    "    \n",
    "    df_easy['typoglycemia'] = df_easy['split'].apply(convert_numbers_to_words)\n",
    "    df_easy = get_typoglycemia_modified_data(df_easy)\n",
    "    df_easy.reset_index(inplace=True, drop=True)\n",
    "    df_easy.to_csv(\"../data/processed/sscorpus_easy2.csv\", index=False)\n",
    "\n",
    "    return df_hard, df_easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(492993, 3)\n",
      "(307038, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adagi\\AppData\\Local\\Temp\\ipykernel_20276\\4189741160.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  new_rows['split'].replace('', np.nan, inplace=True)\n",
      "C:\\Users\\adagi\\AppData\\Local\\Temp\\ipykernel_20276\\4189741160.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  new_rows['split'].replace('', np.nan, inplace=True)\n",
      "C:\\Users\\adagi\\AppData\\Local\\Temp\\ipykernel_20276\\4189741160.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  new_rows['split'].replace('', np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_hard, df_easy = generate_typoglycemia_data_file(0.7, \"../data/raw/sscorpus.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(334954, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hard.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ANLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
